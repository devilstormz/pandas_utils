import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple, Optional
import warnings
warnings.filterwarnings('ignore')

def generate_dummy_data() -> pd.DataFrame:
    """
    Generate dummy data that matches the structure of your Excel file
    
    Returns:
        DataFrame with dummy data similar to your Excel structure
    """
    # Define currencies and types
    currencies = ['AUD', 'CAD', 'CHF', 'EUR', 'GBP', 'ILS', 'JPY', 'MXN', 'NOK', 'SAR', 'USD']
    
    types = [
        'Cumulative Asset',
        'Cumulative Liability', 
        'Cumulative Gap',
        'Net Unencumbered Assets (HQLA 1 / 2A / 2B)'
    ]
    
    tenor_columns = ['1M', '3M', '6M', '12M', '15M', '18M', '21M', '2Y', '3Y', '4Y', '5Y', '>5Y']
    
    # Create dummy data
    data_rows = []
    
    for currency in currencies:
        for type_name in types:
            row = {
                'Currency': currency,
                'Type': type_name
            }
            
            # Generate realistic dummy values for each tenor
            if 'Asset' in type_name:
                # Assets: positive values decreasing over time
                base_value = np.random.uniform(10000, 50000)
                for i, tenor in enumerate(tenor_columns):
                    row[tenor] = max(0, base_value * (0.9 ** i) + np.random.normal(0, 1000))
                    
            elif 'Liability' in type_name:
                # Liabilities: negative values
                base_value = np.random.uniform(-40000, -15000)
                for i, tenor in enumerate(tenor_columns):
                    row[tenor] = min(0, base_value * (0.95 ** i) + np.random.normal(0, 500))
                    
            elif 'Gap' in type_name:
                # Gap: difference between assets and liabilities (can be positive or negative)
                base_value = np.random.uniform(-5000, 15000)
                for i, tenor in enumerate(tenor_columns):
                    row[tenor] = base_value * (0.8 ** i) + np.random.normal(0, 2000)
                    
            elif 'Net Unencumbered' in type_name:
                # Net Unencumbered: mostly positive, smaller values
                base_value = np.random.uniform(100, 2000)
                for i, tenor in enumerate(tenor_columns):
                    row[tenor] = max(0, base_value * (0.7 ** i) + np.random.normal(0, 100))
            
            data_rows.append(row)
    
    # Add Grand Total rows
    grand_total_types = ['HQLA', 'Net Gap', 'Gross Gap']
    
    for gt_type in grand_total_types:
        row = {
            'Currency': f'GRAND TOTAL',
            'Type': gt_type
        }
        
        # Generate grand total values (sum-like values)
        if gt_type == 'HQLA':
            base_value = np.random.uniform(50000, 100000)
        elif gt_type == 'Net Gap':
            base_value = np.random.uniform(-20000, 30000)
        else:  # Gross Gap
            base_value = np.random.uniform(10000, 80000)
            
        for i, tenor in enumerate(tenor_columns):
            row[tenor] = base_value * (0.85 ** i) + np.random.normal(0, 5000)
            
        data_rows.append(row)
    
    # Add some additional rows for incremental calculations
    additional_rows = [
        {
            'Currency': '',
            'Type': 'Incremental unsecured borrowing (LP in bps)',
            **{tenor: np.random.uniform(0, 0.2) for tenor in tenor_columns}
        },
        {
            'Currency': '',
            'Type': 'P&L shock for cost of borrowing - unsecured from Treasury',
            **{tenor: np.random.uniform(-1000, 5000) for tenor in tenor_columns}
        },
        {
            'Currency': '',
            'Type': 'Severe Stress Incremental Cost (in bps)',
            **{tenor: np.random.uniform(0, 0.6) for tenor in tenor_columns}
        },
        {
            'Currency': '',
            'Type': 'Severe Stress P&L Shock (Incremental)',
            **{tenor: np.random.uniform(0, 10000) for tenor in tenor_columns}
        },
        {
            'Currency': '',
            'Type': 'Funding Risk 01',
            **{tenor: np.random.uniform(50000, 200000) for tenor in tenor_columns}
        }
    ]
    
    data_rows.extend(additional_rows)
    
    # Add the Total DSD01 row
    total_dsd01_row = {
        'Currency': '',
        'Type': 'Total DSD01',
        **{tenor: np.random.uniform(100000, 500000) for tenor in tenor_columns}
    }
    data_rows.append(total_dsd01_row)
    
    # Create DataFrame
    df = pd.DataFrame(data_rows)
    
    # Fill NaN values with appropriate defaults
    df = df.fillna(0)
    
    return df

class ExcelDataLoader:
    """Class to handle loading and processing of the multi-indexed Excel file"""
    
    def __init__(self, file_path: str):
        self.file_path = file_path
        
    def load_table(self, start_row: int = 22) -> pd.DataFrame:
        """
        Load the table from Excel starting from specified row (0-indexed)
        
        Args:
            start_row: Row number to start reading from (0-indexed, so 22 for row 23)
        
        Returns:
            DataFrame with properly structured multi-index
        """
        try:
            # Read the Excel file starting from the specified row
            df = pd.read_excel(self.file_path, header=[0, 1], skiprows=start_row)
            
            # Clean up the dataframe
            df = self._clean_dataframe(df)
            
            return df
            
        except Exception as e:
            print(f"Error loading Excel file: {e}")
            return None
    
    def _clean_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:
        """Clean and structure the dataframe properly"""
        
        # Handle merged currency cells by forward filling
        if 'Currency' in df.columns.get_level_values(0):
            currency_col = df.columns.get_level_values(0)[0]
            df[currency_col] = df[currency_col].fillna(method='ffill')
        
        # Clean column names
        df.columns = df.columns.map(lambda x: (x[0] if pd.notna(x[0]) else '', 
                                             x[1] if pd.notna(x[1]) else ''))
        
        # Remove rows that are completely NaN
        df = df.dropna(how='all')
        
        # Reset index
        df = df.reset_index(drop=True)
        
        return df
    
    def filter_grand_total(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Filter for grand total rows from the dataframe
        
        Args:
            df: Input dataframe
            
        Returns:
            DataFrame containing only grand total rows
        """
        # Look for rows containing 'GRAND TOTAL' in any column
        grand_total_mask = df.apply(lambda row: row.astype(str).str.contains('GRAND TOTAL', case=False, na=False).any(), axis=1)
        
        grand_total_df = df[grand_total_mask].copy()
        
        return grand_total_df

class ShockCalculator:
    """Class to handle shock calculations and ladder definitions"""
    
    def __init__(self):
        self.shock_ladders = self._define_shock_ladders()
        self.tenor_days = self._define_tenor_days()
    
    def _define_shock_ladders(self) -> Dict[str, Dict[str, float]]:
        """Define the shock ladders for different scenarios"""
        return {
            'unsecured_borrowing': {
                '1M': 0.00,
                '3M': 0.00,
                '6M': 0.01,
                '12M': 0.07,
                '15M': 0.08,
                '18M': 0.08,
                '21M': 0.09,
                '2Y': 0.10,
                '3Y': 0.14,
                '4Y': 0.16,
                '5Y': 0.18,
                '>5Y': 0.18
            },
            'severe_stress': {
                '1M': 0.50,
                '3M': 0.50,
                '6M': 0.50,
                '12M': 0.50,
                '15M': 0.25,
                '18M': 0.25,
                '21M': 0.25,
                '2Y': 0.25,
                '3Y': 0.0,
                '4Y': 0.0,
                '5Y': 0.0,
                '>5Y': 0.0
            },
            'funding_risk': {
                '1M': 0.005,
                '3M': 0.010,
                '6M': 0.015,
                '12M': 0.020,
                '15M': 0.025,
                '18M': 0.030,
                '21M': 0.035,
                '2Y': 0.040,
                '3Y': 0.050,
                '4Y': 0.060,
                '5Y': 0.070,
                '>5Y': 0.080
            }
        }
    
    def _define_tenor_days(self) -> Dict[str, int]:
        """Define actual days for each tenor bucket"""
        return {
            '1M': 30,
            '3M': 90,
            '6M': 180,
            '12M': 360,
            '15M': 450,
            '18M': 540,
            '21M': 630,
            '2Y': 720,
            '3Y': 1080,
            '4Y': 1440,
            '5Y': 1800,
            '>5Y': 1800  # Assuming >5Y uses 5Y days for calculation
        }
    
    def _calculate_days_difference(self, tenor1: str, tenor2: str) -> int:
        """
        Calculate the difference in days between two tenor buckets
        
        Args:
            tenor1: First tenor (earlier bucket)
            tenor2: Second tenor (later bucket)
            
        Returns:
            Difference in days
        """
        days1 = self.tenor_days.get(tenor1, 0)
        days2 = self.tenor_days.get(tenor2, 0)
        return max(0, days2 - days1)
    
    def calculate_lp_shock(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Calculate LP shock for unsecured borrowing and severe stress
        
        Args:
            df: Input dataframe with net gap data
            
        Returns:
            DataFrame with calculated shocks
        """
        result_df = df.copy()
        
        # Extract tenor columns (assuming they are in the format matching our tenor_days keys)
        tenor_columns = [col for col in df.columns if any(tenor in str(col) for tenor in self.tenor_days.keys())]
        
        # Calculate unsecured borrowing shock
        for col in tenor_columns:
            tenor = self._extract_tenor_from_column(col)
            if tenor in self.shock_ladders['unsecured_borrowing']:
                shock_rate = self.shock_ladders['unsecured_borrowing'][tenor]
                days = self.tenor_days[tenor]
                
                # P&L shock calculation: if net gap > 0, then net gap * shock * days/360 * 1000000, else 0
                result_df[f'{col}_UB_Shock'] = np.where(
                    df[col] > 0,
                    df[col] * shock_rate * days / 360 * 1000000,
                    0
                )
        
        # Calculate severe stress shock
        for col in tenor_columns:
            tenor = self._extract_tenor_from_column(col)
            if tenor in self.shock_ladders['severe_stress']:
                shock_rate = self.shock_ladders['severe_stress'][tenor]
                days = self.tenor_days[tenor]
                
                # Severe stress P&L shock = MAX(net gap * stress shock * days/360, 0)
                result_df[f'{col}_SS_Shock'] = np.maximum(
                    df[col] * shock_rate * days / 360,
                    0
                )
        
        return result_df
    
    def calculate_funding_risk(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Calculate funding risk for each tenor bucket
        
        Formula: MAX(net_gap * funding_shock * difference_in_days_between_tenors / 360 * 1000000, 0)
        
        Args:
            df: Input dataframe with net gap data
            
        Returns:
            DataFrame with funding risk calculations and Total DSD01
        """
        result_df = df.copy()
        
        # Extract tenor columns and sort them by days
        tenor_columns = [col for col in df.columns if any(tenor in str(col) for tenor in self.tenor_days.keys())]
        tenor_mapping = {col: self._extract_tenor_from_column(col) for col in tenor_columns}
        
        # Sort columns by their corresponding days
        sorted_tenor_cols = sorted(tenor_columns, key=lambda x: self.tenor_days.get(tenor_mapping[x], 0))
        
        funding_risk_columns = []
        
        # Calculate funding risk for each tenor bucket
        for i, col in enumerate(sorted_tenor_cols):
            current_tenor = tenor_mapping[col]
            
            if current_tenor in self.shock_ladders['funding_risk']:
                funding_shock = self.shock_ladders['funding_risk'][current_tenor]
                
                # Calculate difference in days between current and next tenor bucket
                if i < len(sorted_tenor_cols) - 1:
                    next_col = sorted_tenor_cols[i + 1]
                    next_tenor = tenor_mapping[next_col]
                    days_diff = self._calculate_days_difference(current_tenor, next_tenor)
                else:
                    # For the last bucket, use the difference from current to a theoretical next bucket
                    # or use the current bucket's days as default
                    days_diff = self.tenor_days[current_tenor]
                
                # Funding Risk calculation: MAX(net gap * funding shock * days_diff / 360 * 1000000, 0)
                funding_risk_col = f'{col}_FR01'
                result_df[funding_risk_col] = np.maximum(
                    df[col] * funding_shock * days_diff / 360 * 1000000,
                    0
                )
                funding_risk_columns.append(funding_risk_col)
        
        # Calculate Total DSD01 as sum of all funding risks
        if funding_risk_columns:
            result_df['Total_DSD01'] = result_df[funding_risk_columns].sum(axis=1)
        
        return result_df
    
    def _extract_tenor_from_column(self, column_name: str) -> str:
        """Extract tenor information from column name"""
        column_str = str(column_name)
        for tenor in self.tenor_days.keys():
            if tenor in column_str:
                return tenor
        return ''

class DiffAnalyzer:
    """Class to create difference analysis and visualizations"""
    
    def __init__(self, base_df: pd.DataFrame):
        self.base_df = base_df
    
    def create_diff_table(self, comparison_df: pd.DataFrame) -> pd.DataFrame:
        """
        Create a difference table between base and comparison dataframes
        
        Args:
            comparison_df: DataFrame to compare against base
            
        Returns:
            DataFrame showing differences
        """
        # Ensure both dataframes have the same structure
        common_columns = self.base_df.columns.intersection(comparison_df.columns)
        
        base_subset = self.base_df[common_columns]
        comp_subset = comparison_df[common_columns]
        
        # Calculate differences (numeric columns only)
        diff_df = pd.DataFrame(index=base_subset.index, columns=common_columns)
        
        for col in common_columns:
            if pd.api.types.is_numeric_dtype(base_subset[col]) and pd.api.types.is_numeric_dtype(comp_subset[col]):
                diff_df[col] = comp_subset[col] - base_subset[col]
            else:
                diff_df[col] = comp_subset[col]  # Keep non-numeric as is
        
        return diff_df
    
    def plot_differences(self, diff_df: pd.DataFrame, title: str = "Difference Analysis") -> None:
        """
        Create visualizations for the difference analysis
        
        Args:
            diff_df: Difference dataframe
            title: Plot title
        """
        # Select numeric columns for plotting
        numeric_cols = diff_df.select_dtypes(include=[np.number]).columns
        
        if len(numeric_cols) == 0:
            print("No numeric columns found for plotting")
            return
        
        # Create subplots
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle(title, fontsize=16)
        
        # Plot 1: Heatmap of differences
        if len(numeric_cols) > 1:
            numeric_data = diff_df[numeric_cols].fillna(0)
            sns.heatmap(numeric_data, annot=True, cmap='RdBu_r', center=0, ax=axes[0,0])
            axes[0,0].set_title('Heatmap of Differences')
        
        # Plot 2: Bar chart of total differences by column
        col_sums = diff_df[numeric_cols].sum()
        col_sums.plot(kind='bar', ax=axes[0,1])
        axes[0,1].set_title('Total Differences by Column')
        axes[0,1].tick_params(axis='x', rotation=45)
        
        # Plot 3: Line chart showing differences across rows
        if len(diff_df) > 1:
            diff_df[numeric_cols].plot(ax=axes[1,0])
            axes[1,0].set_title('Differences Across Rows')
            axes[1,0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        
        # Plot 4: Box plot of differences
        if len(numeric_cols) > 1:
            diff_df[numeric_cols].boxplot(ax=axes[1,1])
            axes[1,1].set_title('Distribution of Differences')
            axes[1,1].tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        plt.show()

# Example usage functions

def load_and_process_data(file_path: str, start_row: int = 22) -> pd.DataFrame:
    """
    Main function to load and process the Excel data
    
    Args:
        file_path: Path to Excel file
        start_row: Row to start reading from (0-indexed)
        
    Returns:
        Processed dataframe
    """
    loader = ExcelDataLoader(file_path)
    df = loader.load_table(start_row)
    return df

def filter_grand_total_data(df: pd.DataFrame) -> pd.DataFrame:
    """
    Filter for grand total data
    
    Args:
        df: Input dataframe
        
    Returns:
        Grand total dataframe
    """
    loader = ExcelDataLoader("")  # Empty path since we're using existing df
    return loader.filter_grand_total(df)

def calculate_shocks(df: pd.DataFrame) -> pd.DataFrame:
    """
    Calculate LP shocks for the dataframe
    
    Args:
        df: Input dataframe
        
    Returns:
        Dataframe with shock calculations
    """
    calculator = ShockCalculator()
    return calculator.calculate_lp_shock(df)

def calculate_funding_risk(df: pd.DataFrame) -> pd.DataFrame:
    """
    Calculate funding risk for the dataframe
    
    Args:
        df: Input dataframe
        
    Returns:
        Dataframe with funding risk calculations
    """
    calculator = ShockCalculator()
    return calculator.calculate_funding_risk(df)

def analyze_differences(base_df: pd.DataFrame, comparison_df: pd.DataFrame, 
                       plot_title: str = "Difference Analysis") -> pd.DataFrame:
    """
    Analyze differences between two dataframes
    
    Args:
        base_df: Base dataframe
        comparison_df: Comparison dataframe
        plot_title: Title for plots
        
    Returns:
        Difference dataframe
    """
    analyzer = DiffAnalyzer(base_df)
    diff_df = analyzer.create_diff_table(comparison_df)
    analyzer.plot_differences(diff_df, plot_title)
    return diff_df

# Example workflow with dummy data
def run_complete_test():
    """
    Complete test workflow using dummy data
    """
    print("=" * 60)
    print("TESTING WITH DUMMY DATA")
    print("=" * 60)
    
    # Generate dummy data
    print("1. Generating dummy data...")
    df = generate_dummy_data()
    print(f"   Generated dataframe shape: {df.shape}")
    print(f"   Columns: {list(df.columns)}")
    print(f"   Sample data:")
    print(df.head(3))
    print()
    
    # Filter for grand total
    print("2. Filtering for grand total data...")
    grand_total_df = filter_grand_total_data(df)
    print(f"   Grand total dataframe shape: {grand_total_df.shape}")
    if not grand_total_df.empty:
        print("   Grand total types found:")
        print(grand_total_df[['Currency', 'Type']].to_string(index=False))
    print()
    
    # Calculate LP shocks
    print("3. Calculating LP shocks...")
    shock_df = calculate_shocks(df)
    shock_columns = [col for col in shock_df.columns if 'Shock' in col]
    print(f"   Added {len(shock_columns)} shock columns:")
    print(f"   {shock_columns[:5]}..." if len(shock_columns) > 5 else f"   {shock_columns}")
    print()
    
    # Calculate funding risk
    print("4. Calculating funding risk...")
    funding_risk_df = calculate_funding_risk(df)
    fr_columns = [col for col in funding_risk_df.columns if 'FR01' in col]
    print(f"   Added {len(fr_columns)} funding risk columns")
    if 'Total_DSD01' in funding_risk_df.columns:
        print("   Total DSD01 calculated successfully")
        sample_total = funding_risk_df['Total_DSD01'].iloc[0]
        print(f"   Sample Total DSD01 value: {sample_total:,.2f}")
    print()
    
    # Create a second dataset for difference analysis
    print("5. Creating comparison dataset and analyzing differences...")
    df2 = generate_dummy_data()
    diff_df = analyze_differences(df, df2, "Test Difference Analysis")
    print(f"   Difference analysis completed")
    print(f"   Difference dataframe shape: {diff_df.shape}")
    print()
    
    # Display sample calculations
    print("6. Sample calculation results:")
    print("-" * 40)
    
    # Show a sample of net gap values and their corresponding shocks
    sample_row = 0
    tenor_cols = ['1M', '3M', '6M', '12M']
    
    print("Sample Net Gap values and calculated shocks:")
    for tenor in tenor_cols:
        if tenor in df.columns:
            net_gap = df[tenor].iloc[sample_row]
            ub_shock_col = f"{tenor}_UB_Shock"
            ss_shock_col = f"{tenor}_SS_Shock"
            fr_shock_col = f"{tenor}_FR01"
            
            print(f"  {tenor}:")
            print(f"    Net Gap: {net_gap:,.2f}")
            
            if ub_shock_col in shock_df.columns:
                ub_shock = shock_df[ub_shock_col].iloc[sample_row]
                print(f"    UB Shock: {ub_shock:,.2f}")
                
            if ss_shock_col in shock_df.columns:
                ss_shock = shock_df[ss_shock_col].iloc[sample_row]
                print(f"    SS Shock: {ss_shock:,.2f}")
                
            if fr_shock_col in funding_risk_df.columns:
                fr_shock = funding_risk_df[fr_shock_col].iloc[sample_row]
                print(f"    Funding Risk: {fr_shock:,.2f}")
    
    print()
    print("=" * 60)
    print("TEST COMPLETED SUCCESSFULLY!")
    print("=" * 60)
    
    return df, grand_total_df, shock_df, funding_risk_df, diff_df

if __name__ == "__main__":
    # Run the complete test
    test_results = run_complete_test()
    
    # You can also test individual components:
    print("\nFor individual testing, you can use:")
    print("df = generate_dummy_data()")
    print("shock_df = calculate_shocks(df)")
    print("funding_df = calculate_funding_risk(df)")
    print("grand_total = filter_grand_total_data(df)")
